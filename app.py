import streamlit as st
import os
import numpy as np
import re
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from groq import Groq

# --- CONFIGURACI√ìN DE LA P√ÅGINA ---
st.set_page_config(page_title="Asistente de Protocolos", page_icon="üö®")

# --- CONFIGURACI√ìN DE API ---
def get_groq_client():
    """Obtiene cliente de Groq con manejo de secretos"""
    try:
        # Intenta usar secretos de HF Spaces
        api_key = st.secrets["GROQ_API_KEY"]
    except:
        # Fallback a clave hardcodeada
        api_key = "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"
    
    return Groq(api_key=api_key)

# --- FUNCIONES DE PROCESAMIENTO ---

def clean_text(text):
    """Limpia el texto extra√≠do de PDFs"""
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\"\']+', ' ', text)
    return text.strip()

def extract_text_with_metadata(pdf_path):
    """Extrae texto con metadatos de estructura"""
    reader = PdfReader(pdf_path)
    sections = []
    
    for page_num, page in enumerate(reader.pages):
        text = page.extract_text()
        if text:
            cleaned_text = clean_text(text)
            
            lines = cleaned_text.split('.')
            for line in lines:
                line = line.strip()
                if len(line) > 10:
                    section = {
                        'text': line,
                        'source_file': os.path.basename(pdf_path),
                        'page': page_num + 1,
                        'is_title': line.isupper() and len(line) < 100
                    }
                    sections.append(section)
    
    return sections

def smart_chunking(sections, chunk_size=800, overlap=150):
    """Chunking inteligente que respeta estructura sem√°ntica"""
    chunks = []
    current_chunk = ""
    current_metadata = {}
    
    for section in sections:
        text = section['text']
        
        if section['is_title'] and current_chunk:
            if len(current_chunk) > 50:
                chunks.append({
                    'text': current_chunk.strip(),
                    'metadata': current_metadata.copy()
                })
            current_chunk = text + " "
            current_metadata = {
                'source_file': section['source_file'],
                'page': section['page'],
                'title': text if section['is_title'] else current_metadata.get('title', '')
            }
        else:
            if len(current_chunk + text) > chunk_size and current_chunk:
                chunks.append({
                    'text': current_chunk.strip(),
                    'metadata': current_metadata.copy()
                })
                
                overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk
                current_chunk = overlap_text + " " + text + " "
            else:
                current_chunk += text + " "
                
                if not current_metadata:
                    current_metadata = {
                        'source_file': section['source_file'],
                        'page': section['page'],
                        'title': ''
                    }
    
    if current_chunk.strip():
        chunks.append({
            'text': current_chunk.strip(),
            'metadata': current_metadata.copy()
        })
    
    return chunks

def expand_query(query):
    """Expande la consulta con t√©rminos relacionados"""
    emergency_keywords = {
        'accidente': ['accidente', 'lesi√≥n', 'herida', 'trauma', 'emergencia m√©dica'],
        'violencia': ['violencia', 'agresi√≥n', 'conflicto', 'bullying', 'acoso'],
        'robo': ['robo', 'hurto', 'sustracci√≥n', 'p√©rdida', 'seguridad'],
        'emergencia': ['emergencia', 'crisis', 'urgencia', 'evacuaci√≥n', 'alarma'],
        'protocolo': ['protocolo', 'procedimiento', 'pasos', 'actuaci√≥n', 'respuesta']
    }
    
    expanded_terms = [query]
    query_lower = query.lower()
    
    for key, synonyms in emergency_keywords.items():
        if key in query_lower:
            expanded_terms.extend(synonyms)
    
    return ' '.join(expanded_terms)

# --- CACHING DE RECURSOS ---
@st.cache_resource
def load_and_process_data(folder_path):
    """Procesa PDFs de la carpeta especificada"""
    st.info("üìö Procesando documentos PDF con t√©cnicas avanzadas...")
    
    all_sections = []
    
    # Verificar que la carpeta existe
    if not os.path.exists(folder_path):
        st.error(f"‚ùå La carpeta '{folder_path}' no existe")
        return None, None, None
    
    # Obtener archivos PDF
    files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]
    
    if not files:
        st.error(f"‚ùå No se encontraron archivos PDF en '{folder_path}'")
        return None, None, None
    
    # Procesar archivos
    progress_bar = st.progress(0)
    
    for i, filename in enumerate(files):
        st.text(f"Procesando: {filename}")
        pdf_path = os.path.join(folder_path, filename)
        try:
            sections = extract_text_with_metadata(pdf_path)
            all_sections.extend(sections)
            st.text(f"‚úÖ {filename}: {len(sections)} secciones extra√≠das")
        except Exception as e:
            st.error(f"‚ùå Error procesando {filename}: {str(e)}")
            continue
        
        progress_bar.progress((i + 1) / len(files))
    
    if not all_sections:
        st.error("‚ùå No se pudo extraer contenido de los PDFs")
        return None, None, None
    
    # Chunking inteligente
    st.text("üìù Organizando contenido en fragmentos sem√°nticos...")
    chunks = smart_chunking(all_sections, chunk_size=800, overlap=150)
    
    # Crear embeddings
    st.text("üß† Creando representaciones vectoriales...")
    try:
        embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        chunk_texts = [chunk['text'] for chunk in chunks]
        embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True, batch_size=16)
        
        st.success(f"‚úÖ Procesados {len(chunks)} fragmentos de {len(files)} documentos!")
        return chunks, embeddings, embedding_model
        
    except Exception as e:
        st.error(f"‚ùå Error creando embeddings: {str(e)}")
        return None, None, None

# --- L√ìGICA DEL CHATBOT ---
def find_relevant_chunks(user_query, chunks, embeddings, model, top_k=7):
    """B√∫squeda mejorada con expansi√≥n de consulta"""
    expanded_query = expand_query(user_query)
    query_embedding = model.encode([expanded_query])
    similarities = cosine_similarity(query_embedding, embeddings)[0]
    
    min_similarity = 0.15
    valid_indices = np.where(similarities > min_similarity)[0]
    
    if len(valid_indices) == 0:
        top_indices = np.argsort(similarities)[-top_k:][::-1]
    else:
        valid_similarities = similarities[valid_indices]
        sorted_valid = np.argsort(valid_similarities)[::-1]
        top_indices = valid_indices[sorted_valid[:min(top_k, len(valid_indices))]]
    
    relevant_chunks = []
    for idx in top_indices:
        chunk_info = chunks[idx].copy()
        chunk_info['similarity'] = similarities[idx]
        relevant_chunks.append(chunk_info)
    
    return relevant_chunks

def generate_response(user_query, context_chunks):
    """Genera respuesta con contexto enriquecido"""
    if not context_chunks:
        return "No encontr√© informaci√≥n espec√≠fica sobre ese tema en los protocolos que tengo disponibles."
    
    try:
        groq_client = get_groq_client()
        
        # Construir contexto enriquecido
        context_text = ""
        for i, chunk in enumerate(context_chunks):
            metadata = chunk['metadata']
            context_text += f"\n--- Fragmento {i+1} (Archivo: {metadata['source_file']}, P√°gina: {metadata['page']}) ---\n"
            if metadata.get('title'):
                context_text += f"Secci√≥n: {metadata['title']}\n"
            context_text += f"{chunk['text']}\n"
        
        prompt = f"""
        Eres un asistente experto en protocolos de actuaci√≥n educativos. Tu tarea es ayudar a docentes en situaciones de emergencia, accidentes, violencia o robos.

        INSTRUCCIONES ESPEC√çFICAS:
        1. Analiza la PREGUNTA del usuario cuidadosamente
        2. Usa EXCLUSIVAMENTE la informaci√≥n del CONTEXTO proporcionado
        3. Si la informaci√≥n est√° disponible, proporciona una respuesta clara y estructurada con:
           - Pasos espec√≠ficos a seguir
           - Personas o autoridades a contactar
           - Acciones inmediatas recomendadas
        4. Si NO hay informaci√≥n suficiente en el contexto, responde: "No encontr√© informaci√≥n espec√≠fica sobre ese tema en los protocolos disponibles."
        5. SIEMPRE indica de qu√© documento proviene la informaci√≥n cuando sea relevante

        --- CONTEXTO DE LOS PROTOCOLOS ---
        {context_text}
        --- FIN DEL CONTEXTO ---

        PREGUNTA DEL DOCENTE: {user_query}

        RESPUESTA PRECISA Y PR√ÅCTICA:
        """
        
        response = groq_client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=1500
        )
        return response.choices[0].message.content
        
    except Exception as e:
        return f"Ocurri√≥ un error al contactar la API de Groq: {e}"

# --- INTERFAZ PRINCIPAL ---
st.title("üö® Asistente de Protocolos de Actuaci√≥n Educativa")
st.write("""
**Sistema especializado en emergencias, accidentes, violencia y robos escolares**

Este asistente ha sido entrenado con los protocolos oficiales de tu instituci√≥n educativa.
Puedes consultar sobre situaciones como:
- ü©π Accidentes y emergencias m√©dicas
- ‚ö†Ô∏è Actos de violencia y conflictos  
- üîí Robos y problemas de seguridad
- üìã Procedimientos de evacuaci√≥n
""")

# Verificar y cargar protocolos
protocols_folder = "protocolos"

if os.path.exists(protocols_folder):
    # Mostrar informaci√≥n de archivos disponibles
    pdf_files = [f for f in os.listdir(protocols_folder) if f.endswith('.pdf')]
    
    with st.expander("üìã Protocolos cargados", expanded=False):
        for pdf_file in pdf_files:
            file_path = os.path.join(protocols_folder, pdf_file)
            file_size = os.path.getsize(file_path) / 1024 / 1024  # MB
            st.write(f"‚Ä¢ **{pdf_file}** ({file_size:.1f} MB)")
    
    # Procesar documentos
    chunks, embeddings, model = load_and_process_data(protocols_folder)
    
    if chunks is not None and embeddings is not None and model is not None:
        # Sidebar con estad√≠sticas
        with st.sidebar:
            st.header("üìä Estado del Sistema")
            st.metric("Documentos procesados", len(pdf_files))
            st.metric("Fragmentos de texto", len(chunks))
            
            st.header("üî• Consultas frecuentes")
            example_queries = [
                "¬øQu√© hacer si un estudiante se accidenta?",
                "Protocolo para casos de violencia escolar",
                "¬øC√≥mo actuar ante un robo en el colegio?",
                "Pasos para evacuar el edificio",
                "¬øA qui√©n contactar en una emergencia?"
            ]
            
            for query in example_queries:
                if st.button(query, key=f"example_{hash(query)}"):
                    # Forzar nueva consulta
                    if "example_query" not in st.session_state:
                        st.session_state.example_query = query
                        st.rerun()
        
        # Sistema de chat
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        # Manejar consulta de ejemplo
        if "example_query" in st.session_state:
            user_input = st.session_state.example_query
            del st.session_state.example_query
        else:
            user_input = None

        # Mostrar historial de chat
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # Input del usuario
        if not user_input:
            user_input = st.chat_input("¬øEn qu√© situaci√≥n necesitas ayuda? Describe el problema...")

        if user_input:
            # Agregar mensaje del usuario
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message("user"):
                st.markdown(user_input)

            # Generar respuesta
            with st.chat_message("assistant"):
                with st.spinner("üîç Consultando protocolos..."):
                    relevant_chunks = find_relevant_chunks(user_input, chunks, embeddings, model)
                    
                    # Debug info
                    with st.expander("üîß Informaci√≥n de b√∫squeda", expanded=False):
                        st.write(f"**Consulta expandida:** {expand_query(user_input)}")
                        st.write(f"**Fragmentos encontrados:** {len(relevant_chunks)}")
                        for i, chunk in enumerate(relevant_chunks):
                            st.write(f"**Fragmento {i+1}** (Similitud: {chunk['similarity']:.3f})")
                            st.write(f"- Archivo: {chunk['metadata']['source_file']}")
                            st.write(f"- P√°gina: {chunk['metadata']['page']}")
                    
                    response = generate_response(user_input, relevant_chunks)
                    st.markdown(response)
            
            st.session_state.messages.append({"role": "assistant", "content": response})
    
    else:
        st.error("‚ùå No se pudieron procesar los documentos. Verifica que los PDFs sean v√°lidos.")

else:
    st.error(f"""
    ‚ùå **Carpeta de protocolos no encontrada**
    
    Se esperaba encontrar la carpeta `{protocols_folder}` con los archivos PDF.
    
    **Estructura esperada:**
    ```
    protocolos/
    ‚îú‚îÄ‚îÄ protocolo1.pdf
    ‚îú‚îÄ‚îÄ protocolo2.pdf  
    ‚îî‚îÄ‚îÄ protocolo3.pdf
    ```
    
    **Para desarrolladores:** Aseg√∫rate de incluir la carpeta `protocolos` con los PDFs en tu repositorio.
    """)

# Footer
st.markdown("---")
st.markdown("*ü§ñ Asistente de Protocolos v2.0 - Powered by Groq & Hugging Face*")
